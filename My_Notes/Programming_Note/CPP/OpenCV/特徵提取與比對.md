# 特徵提取與比對

tags: #cpp/openCV

參考這篇[文章](https://www.cnblogs.com/skyfsm/p/7401523.html)，雖然是簡體字但寫得很詳盡，但都是舊版本的內容，在新版本openCV完全不適用，僅能參考

## Surf

### Sample code

```C++
#include "highgui/highgui.hpp"
#include "opencv2/nonfree/nonfree.hpp"
#include "opencv2/legacy/legacy.hpp"
#include <iostream>

using namespace cv;
using namespace std;


int main()
{
    Mat image01 = imread("2.jpg", 1);    //右圖
    Mat image02 = imread("1.jpg", 1);    //左圖
    namedWindow("p2", 0);
    namedWindow("p1", 0);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度圖轉換
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特徵點
    SurfFeatureDetector surfDetector(800);  // 海塞矩陣閾值，在這裡調整精度，值越大點越少，越精準
    vector<KeyPoint> keyPoint1, keyPoint2;
    surfDetector.detect(image1, keyPoint1);
    surfDetector.detect(image2, keyPoint2);

    //特徵點描述，為下邊的特徵點匹配做準備
    SurfDescriptorExtractor SurfDescriptor;
    Mat imageDesc1, imageDesc2;
    SurfDescriptor.compute(image1, keyPoint1, imageDesc1);
    SurfDescriptor.compute(image2, keyPoint2, imageDesc2);

    //獲得匹配特徵點，並提取最優配對
    FlannBasedMatcher matcher;
    vector<DMatch> matchePoints;

    matcher.match(imageDesc1, imageDesc2, matchePoints, Mat());
    cout << "total match points: " << matchePoints.size() << endl;


    Mat img_match;
    drawMatches(image01, keyPoint1, image02, keyPoint2, matchePoints, img_match);
    namedWindow("match", 0);
    imshow("match",img_match);
    imwrite("match.jpg", img_match);

    waitKey();
    return 0;
}
```

精確版

```C++
#include "highgui/highgui.hpp"
#include "opencv2/nonfree/nonfree.hpp"
#include "opencv2/legacy/legacy.hpp"
#include <iostream>

using namespace cv;
using namespace std;


int main()
{

    Mat image01 = imread("g2.jpg", 1);
    Mat image02 = imread("g4.jpg", 1);
    imshow("p2", image01);
    imshow("p1", image02);

    //灰度圖轉換
    Mat image1, image2;
    cvtColor(image01, image1, CV_RGB2GRAY);
    cvtColor(image02, image2, CV_RGB2GRAY);


    //提取特徵點
    SurfFeatureDetector surfDetector(2000);  // 海塞矩陣閾值，在這裡調整精度，值越大點越少，越精準
    vector<KeyPoint> keyPoint1, keyPoint2;
    surfDetector.detect(image1, keyPoint1);
    surfDetector.detect(image2, keyPoint2);

    //特徵點描述，為下邊的特徵點匹配做準備
    SurfDescriptorExtractor SurfDescriptor;
    Mat imageDesc1, imageDesc2;
    SurfDescriptor.compute(image1, keyPoint1, imageDesc1);
    SurfDescriptor.compute(image2, keyPoint2, imageDesc2);

    FlannBasedMatcher matcher;
    vector<vector<DMatch> > matchePoints;
    vector<DMatch> GoodMatchePoints;

    vector<Mat> train_desc(1, imageDesc1);
    matcher.add(train_desc);
    matcher.train();

    matcher.knnMatch(imageDesc2, matchePoints, 2);
    cout << "total match points: " << matchePoints.size() << endl;

    // Lowe's algorithm,獲取優秀匹配點
    for (int i = 0; i < matchePoints.size(); i++)
    {
        if (matchePoints[i][0].distance < 0.6 * matchePoints[i][1].distance)
        {
            GoodMatchePoints.push_back(matchePoints[i][0]);
        }
    }

    Mat first_match;
    drawMatches(image02, keyPoint2, image01, keyPoint1, GoodMatchePoints, first_match);
    imshow("first_match ", first_match);
    waitKey();
    return 0;
}
```

### My Turn

首先灰階轉換的部分範例中的`cvtColor(image01, image1, CV_RGB2GRAY);`

`cvtColor`的參數定義似乎有所不同已經找不到`CV_RGB2GRAY`了

```C++
CV_EXPORTS_W void cvtColor( InputArray src, OutputArray dst, int code, int dstCn = 0 );

/** @brief Converts an image from one color space to another where the source image is
stored in two planes.

This function only supports YUV420 to RGB conversion as of now.

@param src1: 8-bit image (#CV_8U) of the Y plane.
@param src2: image containing interleaved U/V plane.
@param dst: output image.
@param code: Specifies the type of conversion. It can take any of the following values:
- #COLOR_YUV2BGR_NV12
- #COLOR_YUV2RGB_NV12
- #COLOR_YUV2BGRA_NV12
- #COLOR_YUV2RGBA_NV12
- #COLOR_YUV2BGR_NV21
- #COLOR_YUV2RGB_NV21
- #COLOR_YUV2BGRA_NV21
- #COLOR_YUV2RGBA_NV21
*/
```

找到`ColorConversionCodes`列舉(太多就不貼過來了)裡面的`COLOR_BGR2GRAY`是目前版本openCV中的寫法

另外就是3.0版本以後的openCV在detector這部分的定義有所變更，參考[這裡](https://docs.opencv.org/3.0-beta/doc/user_guide/ug_features2d.html)以及[這裡](https://www.twblogs.net/a/5cff022bbd9eee14029f5a82)

簡單的說現版本的`Detector`和`Descriptor`都是以指標的形式宣告，要使用裡面成員時，則是先取值再呼叫

順利取得`KeyPoint`和`Descriptor`後，進入比較"智慧化"的環節--Match

宣告`FlannBasedMatcher`物件(定義如下)

```C++
/** @brief Flann-based descriptor matcher.

This matcher trains cv::flann::Index on a train descriptor collection and calls its nearest search
methods to find the best matches. So, this matcher may be faster when matching a large train
collection than the brute force matcher. FlannBasedMatcher does not support masking permissible
matches of descriptor sets because flann::Index does not support this. :
 */
class CV_EXPORTS_W FlannBasedMatcher : public DescriptorMatcher
```

```C++
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include<opencv2/xfeatures2d/nonfree.hpp>

#include<iostream>
#include<math.h>
using namespace std;
using namespace cv;
string path = "C:\\Users\\admin\\Desktop\\openCVTestFiles\\";

int main(void)
{
	//Original Mat
	Mat gt1 = imread(path + "GT1.jpg");
	Mat gt2 = imread(path + "GT2.jpg");
	Mat gt3 = imread(path + "GT3.jpg");

	//Grayscale Mat
	Mat gt1g, gt2g, gt3g;
	cvtColor(gt1, gt1g, COLOR_BGR2GRAY);
	cvtColor(gt2, gt2g, COLOR_BGR2GRAY);
	cvtColor(gt3, gt3g, COLOR_BGR2GRAY);

	Ptr<xfeatures2d::SurfFeatureDetector> surfDector = xfeatures2d::SurfFeatureDetector::create(2000);
	vector<KeyPoint> kp1, kp2, kp3;
	surfDector->detect(gt1g, kp1);
	surfDector->detect(gt2g, kp2);
	surfDector->detect(gt3g, kp3);

	Ptr<xfeatures2d::SurfDescriptorExtractor> surfDescriptor;
	Mat desc1, desc2, desc3;
	surfDescriptor->compute(gt1g, kp1, desc1);
	surfDescriptor->compute(gt2g, kp2, desc2);
	surfDescriptor->compute(gt3g, kp3, desc3);

	FlannBasedMatcher matcher;
	vector<vector<DMatch>> matchPoints;
	vector<DMatch> goodMatchPoints;

	vector<Mat> train_desc(1, desc1);
	matcher.add(train_desc);
	matcher.train();

	matcher.knnMatch(desc2, matchPoints, 2);
	cout << "total match points: " << matchPoints.size() << endl;

	for (int i = 0; i < matchPoints.size(); i++)
	{
		if (matchPoints[i][0].distance < 0.6 * matchPoints[i][1].distance)
		{
			goodMatchPoints.push_back(matchPoints[i][0]);
		}
	}

	Mat matchResult;
	drawMatches(gt2, kp2, gt1, kp1, goodMatchPoints, matchResult);
	imshow("Result", matchResult);

	waitKey(0);
	return 0;
}
```

全部搞定後執行至

```C++
Ptr<xfeatures2d::SurfFeatureDetector> surfDector = xfeatures2d::SurfFeatureDetector::create(2000);
```

出現以下錯誤

```
OpenCV(4.5.2-dev) Error: The function/feature is not implemented (This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library) in cv::xfeatures2d::SURF::create, file C:\LIBS\OpenCV\opencv_contrib\modules\xfeatures2d\src\surf.cpp, line 1029
```

查了一下得知surf和sift因為版權問題禁止在商業用途使用，所以opencv在新版本把他們拿掉了...參考[這篇](https://github.com/DynaSlum/satsense/issues/13)

## ORB

2.X版本的範例差的有點多，只能加減參考，就不貼了

### My Turn

Orb同樣和2.X版本寫法有很多不同，不過參照之前的經驗和官方文件還是順利寫出來了，至少它還能用

```C++
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include<opencv2/xfeatures2d/nonfree.hpp>

#include<iostream>
#include<math.h>
using namespace std;
using namespace cv;
string path = "C:\\Users\\admin\\Desktop\\openCVTestFiles\\";

int main(void)
{
	//Original Mat
	Mat gt1 = imread(path + "GT2.jpg");
	Mat gt2 = imread(path + "GT3.jpg");

	if (gt1.empty() || gt2.empty())
		return 0;

	//Grayscale Mat
	Mat gt1g, gt2g;
	cvtColor(gt1, gt1g, COLOR_BGR2GRAY);
	cvtColor(gt2, gt2g, COLOR_BGR2GRAY);

	vector<KeyPoint> kp1, kp2;
	Mat desc1, desc2;
	Ptr<ORB> orb = ORB::create(1000);
	orb->detect(gt1g, kp1);
	orb->detect(gt2g, kp2);

	orb->compute(gt1g, kp1, desc1);
	orb->compute(gt2g, kp2, desc2);

	flann::Index flannIndex(desc1, flann::LshIndexParams(12, 20, 2), cvflann::FLANN_DIST_HAMMING);

	vector<DMatch> goodMatchPoints;

	Mat macthIndex(desc1.rows, 2, CV_32SC1), matchDistance(desc2.rows, 2, CV_32FC1);
	flannIndex.knnSearch(desc2, macthIndex, matchDistance, 2, flann::SearchParams());

	for (int i = 0; i < matchDistance.rows; i++)
	{
		if (matchDistance.at<float>(i, 0) < 0.7 * matchDistance.at<float>(i, 1))
		{
			DMatch dmatches(i, macthIndex.at<int>(i, 0), matchDistance.at<float>(i, 0));
			goodMatchPoints.push_back(dmatches);
		}
	}

	Mat matchResult;
	drawMatches(gt2, kp2, gt1, kp1, goodMatchPoints, matchResult);
	imshow("Result", matchResult);

	waitKey(0);
	return 0;
}
```

註:`ORB::create`沒給參數的話預設是取500點

老實說成果不盡人意，最嚴格的情況下錯誤還是很多，而且match是真的少。後來把標準放到0.7至少線變多了看起來好點。

結果如下: 圖片是goolge隨便找三張金閣寺的照片

![](https://i.imgur.com/HFLKQHO.png)

![](https://i.imgur.com/nsCWtVg.png)

![](https://i.imgur.com/graWRQj.png)

大佛就更慘了

![](https://i.imgur.com/myPQuvq.png)

![](https://i.imgur.com/ixVC0Dr.png)

千本鳥居(好吧我也覺得這個太為難他了)

![](https://i.imgur.com/CprqHVM.png)

## AKAZE

### My Turn

基本上就把上面的ORB換成AKAZE就能用了

值得一提的是create方法沒給參數的話，不會取固定數量的keypoints，比如我的第一個結果中第一張圖取了近2000點第二張圖取了約3000點。

```C++
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include<opencv2/xfeatures2d/nonfree.hpp>

#include<iostream>
#include<math.h>
using namespace std;
using namespace cv;
string path = "C:\\Users\\admin\\Desktop\\openCVTestFiles\\";

int main(void)
{
	string path1, path2;
	cout << "input image1:" << endl;
	cin >> path1;
	cout << "input image2:" << endl;
	cin >> path2;
	//Original Mat
	Mat gt1 = imread(path1);
	Mat gt2 = imread(path2);

	if (gt1.empty() || gt2.empty())
		return 0;

	//Grayscale Mat
	Mat gt1g, gt2g;
	cvtColor(gt1, gt1g, COLOR_BGR2GRAY);
	cvtColor(gt2, gt2g, COLOR_BGR2GRAY);

	vector<KeyPoint> kp1, kp2;
	Mat desc1, desc2;
	Ptr<AKAZE> akaze = AKAZE::create();
	akaze->detect(gt1g, kp1);
	akaze->detect(gt2g, kp2);

	akaze->compute(gt1g, kp1, desc1);
	akaze->compute(gt2g, kp2, desc2);

	flann::Index flannIndex(desc1, flann::LshIndexParams(12, 20, 2), cvflann::FLANN_DIST_HAMMING);

	vector<DMatch> goodMatchPoints;

	Mat macthIndex(desc1.rows, 2, CV_32SC1), matchDistance(desc2.rows, 2, CV_32FC1);
	flannIndex.knnSearch(desc2, macthIndex, matchDistance, 2, flann::SearchParams());

	for (int i = 0; i < matchDistance.rows; i++)
	{
		if (matchDistance.at<float>(i, 0) < 0.6 * matchDistance.at<float>(i, 1))
		{
			DMatch dmatches(i, macthIndex.at<int>(i, 0), matchDistance.at<float>(i, 0));
			goodMatchPoints.push_back(dmatches);
		}
	}

	Mat matchResult;
	drawMatches(gt2, kp2, gt1, kp1, goodMatchPoints, matchResult);
	imshow("AKAZEResult", matchResult);
	imwrite(path + "AKAZEResult\\AKAZEResult01.png", matchResult);
	waitKey(0);
	return 0;
}
```

結果

![](https://i.imgur.com/XiXvi4U.png)

![](https://i.imgur.com/V3E5jOS.png)

![](https://i.imgur.com/q6BTiwA.png)
